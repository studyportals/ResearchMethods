<!DOCTYPE html>
<html lang="en">
   <head>
      <meta charset="UTF-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <title>Research Methods</title>
      <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,600'>
      <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.9/css/all.css" integrity="sha384-5SOiIsAziJl6AWe0HWRKTXlfcSHKmYV4RBF18PPJ173Kzn7jzMyFuTtk8JA7QQG1" crossorigin="anonymous">
      <link rel="stylesheet" href="css/styles.css">
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mixitup/2.1.11/jquery.mixitup.js"></script> 
      <script src="https://cdn.linearicons.com/free/1.0.0/svgembedder.min.js"></script>
   </head>
   <body>

<div class="wrap">
<a href="index.html" class="back">← Overview</a>
</div>

<h1 class="title"><img src="images/usability_test.svg" class="pic"/>Usability Test</h1>  

<div class="article">
   <nav class="menu">
   <ul>
      <li><a class="link2" href="#method1"><b>Moderated usability test</b></a></li>
      <li><a class="link2" href="#method2"><b>Unmoderated usability test</b></a></li>
   </ul>
</nav>

<div class="text">

<p> Can be <b>quantitative</b>, based on users’ performance on a given task (e.g., task-completion times, success rates, number of errors) or can reflect participants’ perception of usability (e.g., satisfaction ratings: for example, the percentage of the participants in a study who were able to complete a task).</p>
<p>Or <b>qualitative</b>, that offers a direct assessment of the usability of a system: researchers will observe participants struggle with specific UI elements and infer which aspects of the design are problematic and which work well.</p> 

<p><hr style="color: #87B7CB;" /></p>

 <section id="method1">
<h3 class="method">Moderated usability test</h3>
<p>Users are brought to the office, with (at least) 2 researchers: one leading the participant(s) through the task(s) and the other one notetaking and observing, to guarantee that the most important insights are well recorded.<br> Most of the times at Studyportals these tests are done remotely, and the researchers follow them from the distance.</p>

<h3 class="method2">When to use:</h3> 
<p>When the designs need to be tested or verified, helps diving into issues through additional questions. </p>

<h3 class="method2">How to use:</h3> 
<p><b>1.</b>  Prepare the test: give scenarios to the users and prepare the task(s) and questions, including possible follow-up questions;<br>
   <h2 class="subtype">Studyportals Good-practices list:</h2> 
<p class="subtype"><b>&#8226;</b> Prepare Excel notetaking file;<br>
<b>&#8226;</b> Check vouchers;<br>
<b>&#8226;</b> Send consent forms and make sure to get them signed before the test takes place;<br>
<b>&#8226;</b> Send reminders via email to users;<br>
<b>&#8226;</b> If the test is remote: add users to Skype;<br>
<b>&#8226;</b> When allowing the company to observe: send Skype for business link;<br>
<b>&#8226;</b> Align with the product researcher on the script;<br>
<b>&#8226;</b> Print the script;<br>
<b>&#8226;</b> Do a pilot test (normally with someone new in the company or who doesn’t know much about our website) to find errors, “bugs” or missing parts;<br>
<b>&#8226;</b> Do the setup for Flashback (our recording system).</p>
<p><b>2.</b>  Keep the participants focused on the test and thinking aloud.</p> 

<h3 class="method2">Advantages:</h3> 
<p><b>-</b> More control from the researchers;<br>
<b>-</b> More specific follow-up questions to each user;<br>
<b>-</b> Gives an overall review.</p>

<h3 class="method2">Disadvantages:</h3> 
<p><b>-</b> More difficulty in scheduling an appointment with the user;<br>
<b>-</b> More time and money are needed (arrange transport, food…).</p>

<h3 class="method2">Tool(s) used at Studyportals:</h3>
<p><a class="link" href="https://www.skype.com" target="_blank">Skype</a>, <a class="link" href="https://www.flashbackrecorder.com/" target="_blank">Flashback</a> and <a class="link" href="https://appear.in/" target="_blank">Appear.in.</a></p>
</section>

<section id="method2">
<h3 class="method">Unmoderated usability test</h3>
<p>An automated method that uses a specialized research tool to capture participant behaviours (through software installed on participant computers/browsers) and attitudes (through embedded survey questions), usually by giving participants goals or scenarios to accomplish with a site or prototype.<p> 

<h3 class="method2">When to use:</h3> 
<p>When the main focus of the study is on a few specific elements and the timeframes are tight.<br>

<h3 class="method2">How to use:</h3> 
<p><b>1.</b>  Prepare the test and the scenarios (the questions should be straight to the point and the instructions clear, since the interviewers cannot interrupt/guide the users during the test.);<br>
<b>2.</b>  Install a tool for remote testing, for example “Trymyui”;<br> 
<b>3.</b>  Add the test with the tasks to the tool to perform the tests. If necessary, predefine follow-up questions to appear after each task, or at the end of the session;<br>
<b>4.</b>  To make sure that the script works and to minimize errors, it's important to do a pilot test (with somebody from the office).</p> 

<h3 class="method2">Advantages:</h3> 
<p>Compared to the moderated usability test setup:<br>
   <b>-</b> No/less effort in recruiting users;<br>
<b>-</b> No time spend in emailing users, sending instructions and setting up Skype;<br>
<b>-</b> No/less technical issues & setting up the devices/tools;<br>
<b>-</b> No meeting room needed;<br>
<b>-</b> No money needs to be arranged for the participants’ transport, neither for snacks or drinks, although more money is needed to compensate the participants;<br>
<b>-</b> No need of researcher(s) during the test.</p>

<h3 class="method2">Disadvantages:</h3>
<p><b>-</b> Isn't possible to ask detailed questions specific to the users’ actions, and they don’t have real-time support (questions, clarification, technology issues);<br>
<b>-</b> If coming from a panel, participants might know how the UI works and do the tests faster (Check if they remember thinking aloud). They might not represent average users anymore;<br>
<b>-</b> If the participant forgets to think aloud no one is there to remind him/her to do it;<br>
<b>-</b> The researcher(s) can't observe the session live and don’t know how the session went until it’s finished;<br>
<b>-</b> It isn't possible to distinguish if the user really understands the task(s) or not. He/she might try until completing the task(s) and eventually complete them by chance and /or persistence. But the system registers them equally as “task completed”;<br>
<b>-</b> After the test, the participants tend to give an over inflated ease of use score about the difficulty of the task(s).</p>
 </section>

</div>
</div>
    
      <script type="text/javascript" src="scripts/main.js"></script>
   </body>
</html>

 
